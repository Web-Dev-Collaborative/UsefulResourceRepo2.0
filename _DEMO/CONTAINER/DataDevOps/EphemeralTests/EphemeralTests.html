<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>EphemeralTests</title>
    <style type="text/css">
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      span.underline {
        text-decoration: underline;
      }
      div.column {
        display: inline-block;
        vertical-align: top;
        width: 50%;
      }
    </style>
  </head>
  <body>
    <h1 id="data-platform-devops---ephemeral-deployments-and-test-environments">
      Data Platform DevOps - Ephemeral Deployments and test environments
    </h1>
    <p><strong>Produced by Dave Lusty</strong></p>
    <h2 id="introduction">Introduction</h2>
    <p>
      This demo shows how to use ephemeral deployments in a data lake
      environment. The video is
      <a href="https://youtu.be/CW5GXIEhePE">not ready yet</a>
    </p>
    <p>
      When creating a DevOps strategy for a data lake the question often comes
      up of how to create test environments as part of a release pipeline. This
      is often phrased “how do I clone my production data lake environment for
      testing or QA?”. The answer is, of course, that you can’t sensibly clone
      your data lake if it contains 10PiB of data. Neither would it be sensible
      to maintain two lake environments since data would gradually drift between
      the two and eventually require a full clone, potentially costing large
      sums of money and time.
    </p>
    <p>
      Instead, it is better to create an ephemeral environment when running your
      release pipeline and destroy it when the release pipeline is completed.
      Needless to say a short lived environment should not spend multiple hours
      copying data, so instead we would use a variable group on the pipeline. In
      your production version of the pipeline, both source and sink will point
      to the production copy of the data. In the case of QA your source(s) will
      still be production data sources, but your sink will be a new storage
      account location dedicated to the test environment. While it would be
      possible to use the production storage account, this brings several
      challenges. First and foremost, it would require the QA environment to
      have read/write access to the production lake. Ideally you should set up a
      new permission on the lake with read only access to ensure that the only
      harm you could do would be if the testing puts a large load on the lake.
      Secondly is removal at the end of testing, and for this removing a storage
      account entirely (or a whole resource group including the QA Data Factory)
      is faster than deleting files. This is especially true of standard blob
      storage where every file must be touched rather than a whole folder.
    </p>
    <figure>
      <img src="images/ephemeraloverview.png" alt="ephemeraloverview" />
      <figcaption>ephemeraloverview</figcaption>
    </figure>
    <p>
      In this scenario, many new environments can be created as needed, often
      one per pipeline for testing purposes.
    </p>
    <p>
      Using this same technique you can create data set versions. In many
      scenarios it might be useful to keep your old data pipeline intact for
      weeks or months after updating it. This allows report versions to continue
      to be supported which depended on an older data model. At the same time
      this reduces the risk of changing pipelines and data models since there is
      a clear and simple roll back mechanism. The obvious downside is that each
      version will cost a similar amount to process.
    </p>
    <figure>
      <img src="images/DataSetVersioning.png" alt="Data set versioning" />
      <figcaption>Data set versioning</figcaption>
    </figure>
  </body>
</html>
